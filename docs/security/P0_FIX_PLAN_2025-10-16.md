# P0 Critical Bug Fix Plan - 2025-10-16

**Status:** In Progress
**Priority:** P0 - Fix TODAY
**Total Bugs:** 14 critical bugs
**Estimated Time:** ~2 hours
**Method:** Parallel agent-based fixes

---

## Executive Summary

This document outlines the fix plan for 14 critical bugs discovered in the mass bug search. These bugs cause:
- Data corruption (audit logs, telemetry metrics)
- Security vulnerabilities (credential leaks)
- System failures (cache broken, resource exhaustion)
- Silent errors (debugging impossible)

**Impact if not fixed:** Production data loss, security breaches, system instability.

---

## Bug Groups (by file for parallelization)

### Group 1: Audit Logger (`osiris/mcp/audit.py`)

**RC-001: Audit Logger Creates New Lock Per Call**
- **Location:** `osiris/mcp/audit.py:257-259`
- **Impact:** Concurrent writes interleave, corrupting JSONL audit logs
- **Fix:** Move `asyncio.Lock()` to `__init__`, reuse instance variable
- **Code Change:**
  ```python
  # In __init__ (around line 28):
  self._write_lock = asyncio.Lock()

  # In _write_event (line 257):
  async with self._write_lock:  # Use instance variable
      with open(self.log_file, "a") as f:
          f.write(json.dumps(event) + "\n")
  ```
- **Test:** Concurrent audit writes should not interleave JSON

---

### Group 2: Telemetry (`osiris/mcp/telemetry.py`)

**RC-002: Telemetry Metrics Race Condition**
- **Location:** `osiris/mcp/telemetry.py:73-78`
- **Impact:** Metrics undercount by 50-70% under concurrent load
- **Fix:** Add `threading.Lock()` around all metrics dict updates
- **Code Change:**
  ```python
  # In __init__ (around line 29):
  import threading
  self._metrics_lock = threading.Lock()

  # In emit_tool_call (lines 73-78):
  with self._metrics_lock:
      self.metrics["tool_calls"] += 1
      self.metrics["total_bytes_in"] += bytes_in
      self.metrics["total_bytes_out"] += bytes_out
      self.metrics["total_duration_ms"] += duration_ms
      if status == "error":
          self.metrics["errors"] += 1
  ```
- **Test:** 100 concurrent tool calls should result in metrics["tool_calls"] == 100

**RC-004: Global Telemetry Lazy Init Race**
- **Location:** `osiris/mcp/telemetry.py:201, 220-221`
- **Impact:** Multiple threads create separate telemetry instances, split event stream
- **Fix:** Add `threading.Lock()` to `init_telemetry()`
- **Code Change:**
  ```python
  _telemetry: TelemetryEmitter | None = None
  _telemetry_lock = threading.Lock()

  def init_telemetry(enabled: bool = True, output_dir: Path | None = None) -> TelemetryEmitter:
      global _telemetry
      with _telemetry_lock:
          if _telemetry is None:
              _telemetry = TelemetryEmitter(enabled, output_dir)
      return _telemetry
  ```
- **Test:** Concurrent `init_telemetry()` calls should create only ONE instance

---

### Group 3: Cache System (`osiris/mcp/cache.py`, `osiris/cli/discovery_cmd.py`)

**RC-003: Cache Dual-Indexing Coherency Bug**
- **Location:** `osiris/mcp/cache.py:90, 160`
- **Impact:** Memory indexed by cache_key, disk by discovery_id → cache always misses
- **Fix:** Use discovery_id consistently for both memory and disk
- **Code Change:**
  ```python
  # In cache.py, line 90 (in get() method):
  # OLD: cache_file = self.cache_dir / f"{cache_key}.json"
  # NEW: Use discovery_id for disk lookup
  discovery_id = generate_discovery_id(connection_id, component_id, samples)
  cache_file = self.cache_dir / f"{discovery_id}.json"

  # Memory cache still uses cache_key for indexing (supports idempotency_key variants)
  # But disk uses discovery_id (canonical identifier)
  ```
- **Test:** Request with idempotency_key should hit disk cache on second call

**CACHE-001: File Path Mismatch Between Write and Read**
- **Location:** `osiris/mcp/cache.py:90 vs 160`
- **Impact:** Persistent cache NEVER works - all disk lookups fail after restart
- **Fix:** Same as RC-003 (use discovery_id for disk operations)
- **Test:** After process restart, cache should still hit from disk

**CACHE-002: TTL Metadata Missing in Memory Cache**
- **Location:** `osiris/mcp/cache.py:54, 78-87`
- **Impact:** Cache bypassed when no idempotency_key provided
- **Fix:** Return full entry dict (with metadata), not just entry["data"]
- **Code Change:**
  ```python
  # In get() method (line 99):
  # OLD: return entry["data"]
  # NEW: return entry  # Include TTL metadata

  # In discovery.py (line 65):
  # Update to extract data field:
  if cached_result:
      discovery_id = cached_result.get("discovery_id")
      data = cached_result.get("data")
      # Use data for actual discovery info
  ```
- **Test:** Cache entry should include "expires_at", "ttl_seconds", "created_at"

---

### Group 4: Memory Tools (`osiris/mcp/tools/memory.py`, `osiris/cli/mcp_cmd.py`)

**URI-001: Memory Tools Use Hardcoded Path.home()**
- **Location:** `osiris/mcp/tools/memory.py:22, 136-140`
- **Impact:** Resource URIs always fail with 404 (resolver can't find files)
- **Fix:** Use config-driven memory_dir instead of Path.home()
- **Code Change:**
  ```python
  # In MemoryTools.__init__ (line 22):
  def __init__(self, memory_dir: Path | None = None):
      if memory_dir is None:
          from osiris.mcp.config import get_config  # noqa: PLC0415
          config = get_config()
          memory_dir = config.memory_dir
      self.memory_dir = memory_dir
  ```
- **Test:** Memory capture should write to config.memory_dir, not ~/.osiris_memory

**URI-002: Memory Tools Bypass CLI-First Security Model**
- **Location:** `osiris/mcp/tools/memory.py:25-99, 140`
- **Impact:** MCP process writes directly to filesystem (security violation)
- **Fix:** Delegate memory capture to CLI via run_cli_json()
- **Code Change:**
  ```python
  # In memory.py, capture() method:
  async def capture(self, args: Dict[str, Any]) -> Dict[str, Any]:
      """Delegate to CLI for memory capture."""
      from osiris.mcp.cli_bridge import run_cli_json

      session_id = args.get("session_id", f"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
      events = args.get("events", [])

      # Delegate to CLI
      result = await run_cli_json([
          "mcp", "memory", "capture",
          "--session-id", session_id,
          "--events", json.dumps(events)
      ])
      return result
  ```
- **Test:** MCP memory capture should call CLI subprocess, not write files directly

---

### Group 5: MySQL Driver (`osiris/drivers/mysql_extractor_driver.py`)

**SECRET-001: MySQL DSN String Construction Leaks Password**
- **Location:** `osiris/drivers/mysql_extractor_driver.py:55`
- **Impact:** Exception before line 57 exposes full DSN with password in traceback
- **Fix:** Create masked URL for logging, real URL only for connection
- **Code Change:**
  ```python
  # Line 55 - Create two separate URLs:
  # Masked URL for potential error messages
  masked_url = f"mysql+pymysql://{user}:***@{host}:{port}/{database}"
  # Real URL for actual connection (don't log this!)
  connection_url = f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}"

  # Use masked_url in any error messages or logging
  # Only use connection_url for engine creation
  ```
- **Test:** Connection failure should NOT show password in logs/stderr

**SECRET-003: MySQL Error Messages Include Connection Details**
- **Location:** `osiris/drivers/mysql_extractor_driver.py:81`
- **Impact:** Database driver exceptions may contain credential fragments
- **Fix:** Generic error message + masked debug logging
- **Code Change:**
  ```python
  # Line 81:
  error_msg = f"MySQL connection failed for step {step_id}"
  logger.error(error_msg)
  # Log detailed error separately with masking
  from osiris.core.secrets_masking import mask_sensitive_string
  logger.debug(f"Connection error details: {mask_sensitive_string(str(e))}")
  ```
- **Test:** Connection errors should not reveal user@host patterns

---

### Group 6: Supabase Driver (`osiris/drivers/supabase_writer_driver.py`)

**SECRET-002: Supabase Writer Logs Plaintext Password Parameters**
- **Location:** `osiris/drivers/supabase_writer_driver.py:677, 682-684, 746`
- **Impact:** IPv4 fallback retry logic exposes password to psycopg2 debug logs
- **Fix:** Remove credential logging, mask connection details
- **Code Change:**
  ```python
  # Line 677/746 - Remove detailed logging:
  # OLD: logger.info(f"Attempting psycopg2 connection to {host} via IPv4: {ipv4}")
  # NEW: Generic message without host details
  logger.info(f"Attempting psycopg2 connection via IPv4 (attempt {idx+1}/{len(ipv4_addresses)})")

  # Lines 682-684 - Connection creation is fine, but ensure password never logged
  conn = psycopg2.connect(
      hostaddr=ipv4,
      port=port,
      user=user,
      password=password,  # Never log this parameter!
      dbname=database,
      sslmode="require",
  )
  # Success log should be generic:
  logger.debug("Connection successful")
  ```
- **Test:** IPv4 fallback should NOT log host, port, or password

**LEAK-002: Supabase psycopg2 IPv4 Attempts Leave Connections Open**
- **Location:** `osiris/drivers/supabase_writer_driver.py:674-691, 744-760`
- **Impact:** Each failed IPv4 attempt leaks connection (10 IPs × 100 discoveries = 900 leaks)
- **Fix:** Close connection in exception handler
- **Code Change:**
  ```python
  # In _connect_psycopg2() method (lines 674-691):
  for ipv4 in ipv4_addresses:
      conn = None  # Initialize to None
      try:
          conn = psycopg2.connect(...)
          return conn  # Success
      except Exception as exc:
          # Close failed connection before continuing
          if conn:
              try:
                  conn.close()
              except:
                  pass  # Connection may not be fully initialized
          logger.warning(f"Connection attempt failed: {exc}")
          # Continue to next IPv4
  ```
- **Test:** Failed connection attempts should not accumulate open connections

---

### Group 7: GraphQL Driver (`osiris/drivers/graphql_extractor_driver.py`)

**LEAK-001: GraphQL Driver Session Never Closed on Exception**
- **Location:** `osiris/drivers/graphql_extractor_driver.py:50-131, 183-187`
- **Impact:** Exception in retry logic leaves session unclosed → socket exhaustion
- **Fix:** Wrap entire try block with session cleanup in finally
- **Code Change:**
  ```python
  # In run() method (around line 50):
  try:
      self.session = self._create_session(config)
      try:
          # Existing query execution logic
          results = self._execute_paginated_query(...)
          return {"extracted_rows": results}
      finally:
          # Always close session
          if self.session:
              self.session.close()
              self.session = None
  except Exception as e:
      # Error handling
      logger.error(f"GraphQL extraction failed: {e}")
      raise
  ```
- **Test:** Exception during pagination should not leak HTTP sessions

---

### Group 8: E2B Adapter (`osiris/remote/e2b_adapter.py`)

**ERR-001: E2B Artifact Download Silent Failures**
- **Location:** `osiris/remote/e2b_adapter.py:706-707`
- **Impact:** Cannot debug E2B execution failures (logs silently missing)
- **Fix:** Log warning on download failure, emit session event
- **Code Change:**
  ```python
  # Line 706-707:
  for file_name in base_files:
      remote_path = f"{remote_base_path}/{file_name}"
      try:
          content = self.client.download_file(self.sandbox_handle, remote_path)
          if content:
              (remote_logs_dir / file_name).write_bytes(content)
              downloaded_count += 1
      except Exception as e:
          # Don't silently pass - log the failure
          logger.warning(f"Failed to download {file_name}: {e}")
          session.log_event("e2b_artifact_download_failed",
                           file_name=file_name,
                           error=str(e))
  ```
- **Test:** Missing artifacts should appear in session logs with warnings

---

## Fix Order & Parallelization Strategy

**Parallel Groups (8 agents):**
1. Agent 1: audit.py (RC-001)
2. Agent 2: telemetry.py (RC-002, RC-004)
3. Agent 3: cache.py + discovery_cmd.py (RC-003, CACHE-001, CACHE-002)
4. Agent 4: memory.py + mcp_cmd.py (URI-001, URI-002)
5. Agent 5: mysql_extractor_driver.py (SECRET-001, SECRET-003)
6. Agent 6: supabase_writer_driver.py (SECRET-002, LEAK-002)
7. Agent 7: graphql_extractor_driver.py (LEAK-001)
8. Agent 8: e2b_adapter.py (ERR-001)

**Sequential Work:**
- After all fixes: Run test suite
- Create fix verification report
- Update CHANGELOG.md

---

## Testing Plan

### Unit Tests (per bug)
```bash
# Concurrency tests
pytest tests/mcp/test_audit_concurrent.py -v
pytest tests/mcp/test_telemetry_concurrent.py -v

# Cache tests
pytest tests/mcp/test_cache_coherency.py -v

# Secret masking tests
pytest tests/drivers/test_secret_masking.py -v

# Resource leak tests
pytest tests/drivers/test_connection_cleanup.py -v
```

### Integration Tests
```bash
# Full test suite
make test

# MCP-specific tests
pytest tests/mcp/ -v

# Expected: 1177+ tests passing, 0 new failures
```

### Manual Verification
```bash
# Verify no secrets in logs
osiris connections list --json | grep -iE "password|token|secret"
# Expected: Only "***MASKED***"

# Verify cache works across restarts
osiris mcp discovery run --connection @mysql.db --component mysql.extractor
# Kill process, restart, run again - should hit cache

# Verify concurrent audit integrity
# Run 100 concurrent MCP tool calls, check audit log for valid JSONL
```

---

## Rollback Plan

If fixes cause test failures:
1. Each agent commits fixes separately
2. Use git to revert individual commits: `git revert <commit-sha>`
3. Re-test after each revert to identify problematic fix
4. Document failed fix for manual review

---

## Success Criteria

- [ ] All 14 P0 bugs fixed
- [ ] Test suite passes (1177+ tests)
- [ ] No new bugs introduced
- [ ] No secrets in JSON outputs (verified)
- [ ] Cache works after restart (verified)
- [ ] Concurrent operations don't corrupt data (verified)
- [ ] Fix summary document created
- [ ] CHANGELOG.md updated

---

## Time Estimates

| Group | Bugs | Est. Time | Agent |
|-------|------|-----------|-------|
| 1. audit.py | 1 | 5 min | Agent 1 |
| 2. telemetry.py | 2 | 10 min | Agent 2 |
| 3. cache.py | 3 | 40 min | Agent 3 |
| 4. memory.py | 2 | 30 min | Agent 4 |
| 5. mysql driver | 2 | 10 min | Agent 5 |
| 6. supabase driver | 2 | 20 min | Agent 6 |
| 7. graphql driver | 1 | 10 min | Agent 7 |
| 8. e2b adapter | 1 | 5 min | Agent 8 |
| **TOTAL** | **14** | **~2 hours** | **8 agents** |

With parallel execution: **~40 minutes wall-clock time** (limited by slowest agent: cache fixes)

---

## Status Tracking

| Bug ID | Status | Agent | Time | Notes |
|--------|--------|-------|------|-------|
| RC-001 | Pending | Agent 1 | - | - |
| RC-002 | Pending | Agent 2 | - | - |
| RC-003 | Pending | Agent 3 | - | - |
| RC-004 | Pending | Agent 2 | - | - |
| CACHE-001 | Pending | Agent 3 | - | - |
| CACHE-002 | Pending | Agent 3 | - | - |
| URI-001 | Pending | Agent 4 | - | - |
| URI-002 | Pending | Agent 4 | - | - |
| SECRET-001 | Pending | Agent 5 | - | - |
| SECRET-002 | Pending | Agent 6 | - | - |
| SECRET-003 | Pending | Agent 5 | - | - |
| LEAK-001 | Pending | Agent 7 | - | - |
| LEAK-002 | Pending | Agent 6 | - | - |
| ERR-001 | Pending | Agent 8 | - | - |

---

## Document Status

**Created:** 2025-10-16
**Status:** Ready for execution
**Next Step:** Launch 8 parallel agents to apply fixes
