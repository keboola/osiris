# Osiris Test Gaps Matrix
*Generated: 2025-09-27*

## Critical Modules Analysis

### üî¥ Remote Module (18.1% coverage)

| Module/File | Coverage | Critical Behaviors | Existing Tests | Missing Scenarios | Suggested Approach |
|-------------|----------|-------------------|----------------|-------------------|-------------------|
| **e2b_transparent_proxy.py** | **0.0%** | ‚Ä¢ Sandbox creation & lifecycle<br>‚Ä¢ RPC communication<br>‚Ä¢ Artifact collection<br>‚Ä¢ Error handling<br>‚Ä¢ Streaming events | None directly<br>‚Ä¢ Parity tests exist but skip proxy | ‚Ä¢ Sandbox creation failure<br>‚Ä¢ RPC timeout handling<br>‚Ä¢ Connection interruption<br>‚Ä¢ Large artifact handling<br>‚Ä¢ Multi-step execution<br>‚Ä¢ Progress streaming | ‚Ä¢ Mock AsyncSandbox<br>‚Ä¢ Mock RPC protocol<br>‚Ä¢ Fixture: `fake_sandbox`<br>‚Ä¢ Test heartbeat mechanism |
| **proxy_worker.py** | **0.0%** | ‚Ä¢ Command reception<br>‚Ä¢ Driver execution<br>‚Ä¢ Event streaming<br>‚Ä¢ Error propagation<br>‚Ä¢ Heartbeat sending | None | ‚Ä¢ All command types<br>‚Ä¢ Driver failures<br>‚Ä¢ Timeout scenarios<br>‚Ä¢ Memory limits<br>‚Ä¢ Concurrent commands | ‚Ä¢ Mock stdin/stdout<br>‚Ä¢ Fake driver registry<br>‚Ä¢ Test in-process<br>‚Ä¢ Capture JSON-RPC |
| **proxy_worker_runner.py** | **0.0%** | ‚Ä¢ Worker initialization<br>‚Ä¢ Main loop execution<br>‚Ä¢ Clean shutdown | None | ‚Ä¢ Entry point testing<br>‚Ä¢ Signal handling<br>‚Ä¢ Graceful shutdown | ‚Ä¢ Mock subprocess<br>‚Ä¢ Test argv parsing |
| **e2b_adapter.py** | **22.1%** | ‚Ä¢ Legacy adapter<br>‚Ä¢ Package building<br>‚Ä¢ Sandbox management | ‚Ä¢ test_e2b_smoke.py<br>‚Ä¢ test_e2b_full_cli.py | ‚Ä¢ Package size limits<br>‚Ä¢ Missing dependencies<br>‚Ä¢ Network failures<br>‚Ä¢ API rate limiting | ‚Ä¢ Being deprecated<br>‚Ä¢ Focus on proxy tests |
| **e2b_client.py** | **25.6%** | ‚Ä¢ SDK wrapper<br>‚Ä¢ API interaction<br>‚Ä¢ Error translation | ‚Ä¢ test_e2b_live.py (skipped) | ‚Ä¢ API errors<br>‚Ä¢ Retry logic<br>‚Ä¢ Timeout handling<br>‚Ä¢ Resource cleanup | ‚Ä¢ Mock E2B SDK<br>‚Ä¢ Fixture: `mock_e2b_api` |
| **rpc_protocol.py** | **99.1%** | ‚Ä¢ Message serialization<br>‚Ä¢ Protocol definition | Well tested | ‚Ä¢ Edge cases only | Already good |

### üî¥ Runtime Module (4.7% coverage)

| Module/File | Coverage | Critical Behaviors | Existing Tests | Missing Scenarios | Suggested Approach |
|-------------|----------|-------------------|----------------|-------------------|-------------------|
| **local_adapter.py** | **4.7%** | ‚Ä¢ Driver loading<br>‚Ä¢ Step execution<br>‚Ä¢ Data flow<br>‚Ä¢ Metrics collection<br>‚Ä¢ Error handling | ‚Ä¢ test_local_e2e_with_cfg.py (1 test) | ‚Ä¢ Driver not found<br>‚Ä¢ Driver exceptions<br>‚Ä¢ Data type mismatches<br>‚Ä¢ Large dataframes<br>‚Ä¢ Concurrent execution<br>‚Ä¢ Memory constraints | ‚Ä¢ Mock DriverRegistry<br>‚Ä¢ Fake drivers<br>‚Ä¢ Fixture: `mock_drivers`<br>‚Ä¢ Test data generators |

### üü° CLI Module (32.9% coverage)

| Module/File | Coverage | Critical Behaviors | Existing Tests | Missing Scenarios | Suggested Approach |
|-------------|----------|-------------------|----------------|-------------------|-------------------|
| **cli_orchestrator.py** | **38.5%** | ‚Ä¢ Command dispatch<br>‚Ä¢ State management<br>‚Ä¢ Error recovery | Basic tests | ‚Ä¢ Command chaining<br>‚Ä¢ Interrupt handling<br>‚Ä¢ State corruption<br>‚Ä¢ Concurrent commands | ‚Ä¢ Mock Rich console<br>‚Ä¢ Capture output<br>‚Ä¢ Test state machine |
| **compile.py** | **42.0%** | ‚Ä¢ Pipeline compilation<br>‚Ä¢ Validation<br>‚Ä¢ Output generation | ‚Ä¢ test_all_commands_json.py | ‚Ä¢ Invalid YAML<br>‚Ä¢ Missing connections<br>‚Ä¢ Circular dependencies<br>‚Ä¢ Large pipelines | ‚Ä¢ Test fixtures<br>‚Ä¢ Error scenarios |
| **chat.py** | **58.8%** | ‚Ä¢ Conversation flow<br>‚Ä¢ LLM interaction<br>‚Ä¢ Session management | ‚Ä¢ test_chat.py<br>‚Ä¢ test_chat_session.py | ‚Ä¢ LLM failures<br>‚Ä¢ Session recovery<br>‚Ä¢ Multi-turn dialog<br>‚Ä¢ Context overflow | ‚Ä¢ Mock LLM adapter<br>‚Ä¢ Conversation fixtures |
| **main.py** | **56.9%** | ‚Ä¢ Entry point<br>‚Ä¢ Argument parsing<br>‚Ä¢ Command routing | ‚Ä¢ test_all_commands_json.py | ‚Ä¢ Invalid arguments<br>‚Ä¢ Environment setup<br>‚Ä¢ Signal handling | ‚Ä¢ Mock sys.argv<br>‚Ä¢ Test Click commands |

### üü° Core Module (70.8% coverage - but key files need work)

| Module/File | Coverage | Critical Behaviors | Existing Tests | Missing Scenarios | Suggested Approach |
|-------------|----------|-------------------|----------------|-------------------|-------------------|
| **prompt_manager.py** | **13.1%** | ‚Ä¢ Template rendering<br>‚Ä¢ Context building<br>‚Ä¢ Prompt optimization | Minimal | ‚Ä¢ All template types<br>‚Ä¢ Large contexts<br>‚Ä¢ Token limits<br>‚Ä¢ Variable substitution | ‚Ä¢ Template fixtures<br>‚Ä¢ Mock tokenizer<br>‚Ä¢ Test all providers |
| **llm_adapter.py** | **59.6%** | ‚Ä¢ Multi-provider support<br>‚Ä¢ API calls<br>‚Ä¢ Response parsing<br>‚Ä¢ Retry logic | ‚Ä¢ test_llm_adapter.py | ‚Ä¢ Provider failures<br>‚Ä¢ Rate limiting<br>‚Ä¢ Streaming responses<br>‚Ä¢ Token counting | ‚Ä¢ Mock API clients<br>‚Ä¢ Response fixtures<br>‚Ä¢ Error scenarios |
| **conversational_agent.py** | **50.8%** | ‚Ä¢ FSM transitions<br>‚Ä¢ State persistence<br>‚Ä¢ Intent recognition | ‚Ä¢ test_chat_*.py files | ‚Ä¢ State recovery<br>‚Ä¢ Invalid transitions<br>‚Ä¢ Context limits<br>‚Ä¢ Parallel conversations | ‚Ä¢ State fixtures<br>‚Ä¢ Mock state store<br>‚Ä¢ FSM test harness |

### üü¢ Well-Tested Modules (>70% coverage)

| Module | Coverage | Notes |
|--------|----------|-------|
| **drivers/** | 76.0% | Good driver coverage |
| **core/secrets_masking.py** | 100% | Complete coverage |
| **core/state_store.py** | 100% | Complete coverage |
| **components/** | 62.6% | Acceptable, needs edge cases |

## Priority Gaps by Impact

### üö® P0 - Critical Gaps (Blocks Production)
1. **E2B Transparent Proxy** - 0% coverage of new architecture
2. **ProxyWorker** - 0% coverage of sandbox execution
3. **LocalAdapter** - 4.7% coverage of execution path

### ‚ö†Ô∏è P1 - High Priority (User-Facing)
1. **PromptManager** - 13% coverage, critical for LLM interaction
2. **CLI Orchestrator** - 38% coverage of user commands
3. **Compile Command** - 42% coverage of pipeline generation

### üìù P2 - Medium Priority (Stability)
1. **LLM Adapter** - Missing provider-specific tests
2. **Conversational Agent** - FSM edge cases
3. **Chat CLI** - Multi-turn conversation tests

## Recommended Test Fixtures

### For E2B/Remote Testing
```python
@pytest.fixture
def mock_sandbox():
    """Mock E2B sandbox with controllable behavior"""

@pytest.fixture
def fake_rpc_channel():
    """In-memory RPC channel for testing protocol"""

@pytest.fixture
def proxy_worker_harness():
    """Test harness for ProxyWorker without subprocess"""
```

### For Runtime Testing
```python
@pytest.fixture
def mock_driver_registry():
    """Registry with fake drivers for testing"""

@pytest.fixture
def fake_execution_context():
    """Context with temp directories and session"""

@pytest.fixture
def sample_manifests():
    """Collection of test manifests"""
```

### For CLI Testing
```python
@pytest.fixture
def cli_runner():
    """Click test runner with captured output"""

@pytest.fixture
def mock_rich_console():
    """Rich console that captures formatted output"""

@pytest.fixture
def temp_config_env():
    """Temporary configuration environment"""
```

## Test Implementation Priorities

### Week 1: E2B Foundation (Expected +8% coverage)
- Mock E2B SDK and AsyncSandbox
- Test E2BTransparentProxy lifecycle
- Test RPC protocol with mock channels
- Test ProxyWorker message handling

### Week 2: Runtime & Execution (Expected +6% coverage)
- Mock driver registry and drivers
- Test LocalAdapter execution flow
- Test error propagation and recovery
- Test metrics collection

### Week 3: CLI & User Experience (Expected +5% coverage)
- Test all CLI commands with fixtures
- Test error messages and help text
- Test configuration handling
- Test session management

### Week 4: Core Logic & Integration (Expected +4% coverage)
- Test prompt templates and rendering
- Test LLM provider failover
- Test conversation state machine
- End-to-end integration tests

## Success Metrics

| Metric | Current | Target | Milestone |
|--------|---------|--------|-----------|
| Overall Coverage | 22.87% | 50% | 3 months |
| E2B/Remote Coverage | 18.1% | 80% | Week 1 |
| Runtime Coverage | 4.7% | 70% | Week 2 |
| CLI Coverage | 32.9% | 70% | Week 3 |
| Test Execution Time | 60s | <90s | Ongoing |
| Flaky Test Rate | Unknown | <2% | Week 4 |
